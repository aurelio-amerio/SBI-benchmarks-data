{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9026d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sbibm\n",
    "import jax \n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import json\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d404b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"aurelio-amerio/SBI-benchmarks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658bb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoresbibm.tasks.sbibm_tasks import (\n",
    "    LinearGaussian,\n",
    "    BernoulliGLM,\n",
    "    BernoulliGLMRaw,\n",
    "    MixtureGaussian,\n",
    "    TwoMoons,\n",
    "    SLCP)\n",
    "    \n",
    "from scoresbibm.tasks.unstructured_tasks import (\n",
    "    LotkaVolterraTask,\n",
    "    SIRTask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2a02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_task_cls = [LinearGaussian,\n",
    "              BernoulliGLM,\n",
    "              BernoulliGLMRaw,\n",
    "              MixtureGaussian,\n",
    "              TwoMoons,\n",
    "              SLCP]\n",
    "\n",
    "advanced_task_cls = [LotkaVolterraTask,\n",
    "                  SIRTask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42063679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "metadata = {}\n",
    "\n",
    "pbar = tqdm.tqdm(total=len(base_task_cls) + len(advanced_task_cls))\n",
    "for task_cls in base_task_cls:\n",
    "    task = task_cls()\n",
    "    task_name = task.name\n",
    "    dim_data = task.get_x_dim()\n",
    "    dim_theta = task.get_theta_dim()\n",
    "\n",
    "    metadata[task_name] = {\"dim_data\": dim_data, \"dim_theta\": dim_theta, \"metadata\": None}\n",
    "    pbar.update(1)\n",
    "\n",
    "# for task_cls in advanced_task_cls:\n",
    "#     task = task_cls()\n",
    "#     task_name = task.name\n",
    "#     dim_data = task.get_x_dim()\n",
    "#     dim_theta = task.get_theta_dim()\n",
    "\n",
    "#     samples = task.get_data(10, jax.random.PRNGKey(0))\n",
    "#     metadata_ = np.array(samples[\"metadata\"]).tolist()\n",
    "\n",
    "#     metadata[task_name] = {\"dim_data\": dim_data, \"dim_theta\": dim_theta, \"metadata\": metadata_}\n",
    "#     pbar.update(1)\n",
    "\n",
    "file_path = \"metadata.json\"\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb71f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_data_base(task_cls, num_samples):\n",
    "    task = task_cls()\n",
    "    data = task.get_data(num_samples)\n",
    "    reference_posteriors = []\n",
    "    true_parameters = []\n",
    "    observations = []\n",
    "    for i in range(1,11):\n",
    "        reference_posteriors.append(task.get_reference_posterior_samples(i))\n",
    "        true_parameters.append(task.get_true_parameters(i))\n",
    "        observations.append(task.get_observation(i))\n",
    "\n",
    "    return data, reference_posteriors, true_parameters, observations\n",
    "\n",
    "# def get_task_data_advanced(task_cls, num_samples):\n",
    "#     task = task_cls()\n",
    "\n",
    "#     data = task.get_data(num_samples,jax.random.PRNGKey(42))\n",
    "    \n",
    "#     # posterior_sampler = task.get_reference_sampler()\n",
    "#     observation_generator = task.get_observation_generator(\"posterior\") \n",
    "\n",
    "#     itr = observation_generator(jax.random.PRNGKey(42))\n",
    "\n",
    "#     for i in range(1,11):\n",
    "#         condition_mask, x_o, theta_o, meta_data, node_ids = next(itr)\n",
    "#         observations.append(task.get_observation(i))\n",
    "\n",
    "#     reference_posteriors = []\n",
    "#     true_parameters = []\n",
    "#     observations = []\n",
    "#     for i in range(1,11):\n",
    "#         # reference_posteriors.append(task.get_reference_posterior_samples(i))\n",
    "#         true_parameters.append(task.get_true_parameters(i))\n",
    "#         observations.append(task.get_observation(i))\n",
    "\n",
    "#     return data, reference_posteriors, true_parameters, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ffcd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/aurelio-amerio/SBI-benchmarks/commit/19cbce6ebefb0ab96d743011465196869722c5d1', commit_message='Upload metadata.json with huggingface_hub', commit_description='', oid='19cbce6ebefb0ab96d743011465196869722c5d1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/aurelio-amerio/SBI-benchmarks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='aurelio-amerio/SBI-benchmarks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_file(\n",
    "    path_or_fileobj=file_path,\n",
    "    path_in_repo=\"metadata.json\",  # The name of the file in the repo\n",
    "    repo_id=repo_name,\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a8c3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset(task_cls, repo_name: str):\n",
    "\n",
    "    task = task_cls()\n",
    "    task_name = task.name\n",
    "    max_samples = int(1e6)\n",
    "    num_samples = max_samples + 1000\n",
    "\n",
    "    data_dict, reference_posteriors, true_parameters, observations = get_task_data_base(task_cls, num_samples)\n",
    "    \n",
    "    dtype = np.float32\n",
    "\n",
    "    xs = data_dict[\"x\"][: max_samples]\n",
    "    xs = np.array(xs).astype(dtype)\n",
    "    thetas = data_dict[\"theta\"][: max_samples]\n",
    "    thetas = np.array(thetas).astype(dtype)\n",
    "\n",
    "    xs_val = data_dict[\"x\"][max_samples :]\n",
    "    xs_val = np.array(xs_val).astype(dtype)\n",
    "    thetas_val = data_dict[\"theta\"][max_samples :]\n",
    "    thetas_val = np.array(thetas_val).astype(dtype)\n",
    "\n",
    "    observations = np.array(observations).astype(dtype)\n",
    "\n",
    "    reference_samples = np.array(reference_posteriors)\n",
    "    reference_samples = reference_samples.astype(dtype)\n",
    "\n",
    "    true_parameters = np.array(true_parameters).astype(dtype)\n",
    "\n",
    "    # dim_data = data_dict[\"dim_data\"]\n",
    "    # dim_theta = data_dict[\"dim_theta\"]\n",
    "    # dim_joint = dim_data + dim_theta\n",
    "    # num_observations = data_dict[\"num_observations\"]\n",
    "\n",
    "    dataset_train = Dataset.from_dict({\"xs\": xs, \"thetas\": thetas})\n",
    "    dataset_val = Dataset.from_dict({\"xs\": xs_val, \"thetas\": thetas_val})\n",
    "    dataset_reference_posterior = Dataset.from_dict(\n",
    "        {\"reference_samples\": reference_samples, \"observations\": observations, \"true_parameters\": true_parameters}\n",
    "    )\n",
    "\n",
    "    dataset_train.push_to_hub(repo_name, config_name=task_name, split=\"train\", private=False)\n",
    "    dataset_val.push_to_hub(repo_name, config_name=task_name, split=\"validation\", private=False)\n",
    "    dataset_reference_posterior.push_to_hub(repo_name, config_name=f\"{task_name}_posterior\", split=\"reference_posterior\", private=False)\n",
    "\n",
    "    return #dataset_train, dataset_val, dataset_reference_posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35a019",
   "metadata": {},
   "source": [
    "# upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c62acd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  3.13ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 81.3MB / 81.3MB, 6.43MB/s  \n",
      "New Data Upload: 100%|██████████| 81.3MB / 81.3MB, 6.43MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:17<00:00, 17.74s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 459.20ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  117kB /  117kB,  195kB/s  \n",
      "New Data Upload: 100%|██████████|  117kB /  117kB,  195kB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.31s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 30.47ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 4.70MB / 4.70MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.03 shards/s]\n",
      "\n",
      "\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  2.88ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 81.3MB / 81.3MB, 7.51MB/s  \n",
      "New Data Upload: 100%|██████████| 81.3MB / 81.3MB, 7.51MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:13<00:00, 13.38s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 666.08ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  113kB /  113kB,  189kB/s  \n",
      "New Data Upload: 100%|██████████|  113kB /  113kB,  189kB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.36s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 33.95ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 4.70MB / 4.70MB,  847kB/s  \n",
      "New Data Upload: 100%|██████████| 4.70MB / 4.70MB,  847kB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.55s/ shards]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:02<00:00,  2.50ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 57.5MB / 57.5MB, 7.78MB/s  \n",
      "New Data Upload: 100%|██████████| 57.5MB / 57.5MB, 7.78MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:19<00:00, 19.82s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 369.02ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.67 shards/s]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 35.26ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 4.70MB / 4.70MB, 96.6kB/s  \n",
      "New Data Upload: 100%|██████████| 38.6kB / 38.6kB, 96.6kB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.17s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 10.84ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 17.2MB / 17.2MB, 5.12MB/s  \n",
      "New Data Upload: 100%|██████████| 17.2MB / 17.2MB, 5.12MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.50s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 858.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.67 shards/s]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 52.66ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 1.25MB / 1.25MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.34 shards/s]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 10.68ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 17.2MB / 17.2MB, 3.58MB/s  \n",
      "New Data Upload: 100%|██████████| 17.2MB / 17.2MB, 3.58MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1808.67ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.01 shards/s]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 51.00ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 1.25MB / 1.25MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.31 shards/s]\n",
      "\n",
      "\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  4.87ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 53.3MB / 53.3MB, 8.59MB/s  \n",
      "New Data Upload: 100%|██████████| 53.3MB / 53.3MB, 8.59MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.35s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 934.98ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.89s/ shards]\n",
      "\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 39.12ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 2.70MB / 2.70MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.06 shards/s]\n"
     ]
    }
   ],
   "source": [
    "for task_cls in base_task_cls:\n",
    "    upload_dataset(task_cls, repo_name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079d970",
   "metadata": {},
   "source": [
    "# gw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb79094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2e2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_dir = \"/home/aure/Documents/dataset(1)/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "643095d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = torch.load(f\"{gw_dir}/thetas_0.pt\")\n",
    "theta1 = torch.load(f\"{gw_dir}/thetas_1.pt\")\n",
    "theta2 = torch.load(f\"{gw_dir}/thetas_2.pt\")\n",
    "theta3 = torch.load(f\"{gw_dir}/thetas_3.pt\")\n",
    "theta4 = torch.load(f\"{gw_dir}/thetas_4.pt\")\n",
    "theta5 = torch.load(f\"{gw_dir}/thetas_5.pt\")\n",
    "theta6 = torch.load(f\"{gw_dir}/thetas_6.pt\")\n",
    "theta7 = torch.load(f\"{gw_dir}/thetas_7.pt\")\n",
    "theta8 = torch.load(f\"{gw_dir}/thetas_8.pt\")\n",
    "theta9 = torch.load(f\"{gw_dir}/thetas_9.pt\")\n",
    "\n",
    "\n",
    "\n",
    "xs_raw = torch.load(f\"{gw_dir}/xs_0.pt\")\n",
    "xs_raw1 = torch.load(f\"{gw_dir}/xs_1.pt\")\n",
    "xs_raw2 = torch.load(f\"{gw_dir}/xs_2.pt\")\n",
    "xs_raw3 = torch.load(f\"{gw_dir}/xs_3.pt\")\n",
    "xs_raw4 = torch.load(f\"{gw_dir}/xs_4.pt\")\n",
    "xs_raw5 = torch.load(f\"{gw_dir}/xs_5.pt\")\n",
    "xs_raw6 = torch.load(f\"{gw_dir}/xs_6.pt\")\n",
    "xs_raw7 = torch.load(f\"{gw_dir}/xs_7.pt\")\n",
    "xs_raw8 = torch.load(f\"{gw_dir}/xs_8.pt\")\n",
    "xs_raw9 = torch.load(f\"{gw_dir}/xs_9.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = torch.cat([thetas, theta1, theta2, theta3, theta4, theta5, theta6, theta7, theta8, theta9], dim=0)\n",
    "xs_raw = torch.cat([xs_raw, xs_raw1, xs_raw2, xs_raw3, xs_raw4, xs_raw5, xs_raw6, xs_raw7, xs_raw8, xs_raw9], dim=0)\n",
    "\n",
    "\n",
    "thetas = jnp.array(thetas.numpy())\n",
    "xs_raw = jnp.array(xs_raw.numpy())\n",
    "\n",
    "thetas = jax.device_put(thetas, jax.devices(\"cpu\")[0])\n",
    "xs_raw = jax.device_put(xs_raw, jax.devices(\"cpu\")[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d575ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_dict({\"xs\": xs_raw, \"thetas\": thetas})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
